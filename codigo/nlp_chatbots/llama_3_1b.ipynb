{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#! pip install ipywidgets\n",
    "#!pip install -U accelerate\n",
    "#!pip install -U bitsandbytes\n",
    "#!pip install  -U peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Any\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Global variables\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "MAX_LENGTH = 256  # Reduced from 512\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Clear CUDA cache at start\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check:\n",
      "=================\n",
      "PyTorch version: 2.4.0+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce GTX 1050\n",
      "CUDA memory allocated: 0.00 MB\n",
      "CUDA memory cached: 0.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_environment():\n",
    "    \"\"\"Check if the environment is properly set up.\"\"\"\n",
    "    print(\"Environment Check:\")\n",
    "    print(\"=================\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "        print(f\"CUDA memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "# Check environment\n",
    "check_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "def login_to_huggingface(token=None):\n",
    "    \"\"\"\n",
    "    Log into Hugging Face using an access token.\n",
    "    If no token is provided, it will look for the 'HUGGINGFACE_TOKEN' environment variable.\n",
    "    \n",
    "    You can get your token from: https://huggingface.co/settings/tokens\n",
    "    \"\"\"\n",
    "    if token is None:\n",
    "        # Try to get token from environment variable\n",
    "        token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "        if token is None:\n",
    "            raise ValueError(\"Please provide a token or set the HUGGINGFACE_TOKEN environment variable\")\n",
    "    \n",
    "    # Login to Hugging Face\n",
    "    login(token=token)\n",
    "    print(\"Successfully logged into Hugging Face!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/uumami/.cache/huggingface/token\n",
      "Login successful\n",
      "Successfully logged into Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "login_to_huggingface(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Information:\n",
      "================\n",
      "GPU Device: NVIDIA GeForce GTX 1050\n",
      "Total Memory: 3.94 GB\n",
      "CUDA Version: 12.1\n",
      "CUDA Arch List: 5\n"
     ]
    }
   ],
   "source": [
    "def setup_device():\n",
    "    \"\"\"\n",
    "    Setup and verify device (GPU/CPU) availability and properties.\n",
    "    Returns the appropriate device and its properties.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Get GPU properties\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_properties = torch.cuda.get_device_properties(device)\n",
    "        total_memory = gpu_properties.total_memory / (1024**3)  # Convert to GB\n",
    "        \n",
    "        print(\"\\nGPU Information:\")\n",
    "        print(\"================\")\n",
    "        print(f\"GPU Device: {torch.cuda.get_device_name(device)}\")\n",
    "        print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"CUDA Arch List: {gpu_properties.multi_processor_count}\")\n",
    "        \n",
    "        # Clear any existing cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return device, total_memory\n",
    "    else:\n",
    "        print(\"\\nNo GPU detected, using CPU\")\n",
    "        return torch.device(\"cpu\"), None\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "device = setup_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_examples(dataset, num_examples=3):\n",
    "    \"\"\"\n",
    "    Enhanced example examination with proper index type handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert numpy int64 to standard Python int\n",
    "        indices = [int(idx) for idx in np.random.choice(len(dataset['train']), num_examples, replace=False)]\n",
    "        \n",
    "        print(\"\\nExamining Random Examples:\")\n",
    "        print(\"========================\")\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            example = dataset['train'][idx]\n",
    "            \n",
    "            print(f\"\\nExample {i+1}/{num_examples}:\")\n",
    "            print(\"=\" * 40)\n",
    "            print(f\"Category: {example.get('category', 'N/A')}\")\n",
    "            print(\"\\nInstruction:\")\n",
    "            print(\"-\" * 20)\n",
    "            print(example['instruction'])\n",
    "            print(\"\\nResponse:\")\n",
    "            print(\"-\" * 20)\n",
    "            print(example['response'])\n",
    "            \n",
    "            # Additional analysis\n",
    "            instruction_words = len(example['instruction'].split())\n",
    "            response_words = len(example['response'].split())\n",
    "            print(\"\\nMetrics:\")\n",
    "            print(f\"- Instruction length: {instruction_words} words\")\n",
    "            print(f\"- Response length: {response_words} words\")\n",
    "            print(f\"- Response/Instruction ratio: {response_words/instruction_words:.2f}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in example examination: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def explore_dataset(dataset_name=\"databricks/databricks-dolly-15k\", validation_split=0.1):\n",
    "    \"\"\"\n",
    "    Enhanced dataset exploration with proper index type handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load dataset with error handling\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        print(\"\\nOriginal Dataset Structure:\")\n",
    "        print(\"=========================\")\n",
    "        print(dataset)\n",
    "        \n",
    "        # Memory-efficient shuffling using indices\n",
    "        dataset_size = len(dataset['train'])\n",
    "        indices = np.random.permutation(dataset_size)\n",
    "        val_size = int(dataset_size * validation_split)\n",
    "        \n",
    "        # Convert numpy indices to Python integers\n",
    "        train_indices = [int(idx) for idx in indices[val_size:]]\n",
    "        val_indices = [int(idx) for idx in indices[:val_size]]\n",
    "        \n",
    "        # Split dataset\n",
    "        split_dataset = {\n",
    "            'train': dataset['train'].select(train_indices),\n",
    "            'validation': dataset['train'].select(val_indices)\n",
    "        }\n",
    "        \n",
    "        # Calculate statistics with progress bar\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        def calculate_lengths(examples):\n",
    "            instruction_lengths = []\n",
    "            response_lengths = []\n",
    "            total_tokens = 0\n",
    "            \n",
    "            for ex in tqdm(examples, desc=\"Analyzing examples\"):\n",
    "                instruction_lengths.append(len(ex['instruction'].split()))\n",
    "                response_lengths.append(len(ex['response'].split()))\n",
    "                total_tokens += len(ex['instruction'].split()) + len(ex['response'].split())\n",
    "                \n",
    "            return {\n",
    "                'instruction_lengths': instruction_lengths,\n",
    "                'response_lengths': response_lengths,\n",
    "                'total_tokens': total_tokens\n",
    "            }\n",
    "        \n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(\"==================\")\n",
    "        print(f\"Total examples: {dataset_size}\")\n",
    "        print(f\"Training examples: {len(split_dataset['train'])}\")\n",
    "        print(f\"Validation examples: {len(split_dataset['validation'])}\")\n",
    "        \n",
    "        stats = calculate_lengths(split_dataset['train'])\n",
    "        \n",
    "        avg_instruction_len = np.mean(stats['instruction_lengths'])\n",
    "        avg_response_len = np.mean(stats['response_lengths'])\n",
    "        print(f\"\\nAverage instruction length (words): {avg_instruction_len:.1f}\")\n",
    "        print(f\"Average response length (words): {avg_response_len:.1f}\")\n",
    "        \n",
    "        # Enhanced visualizations\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.hist(stats['instruction_lengths'], bins=50, alpha=0.7)\n",
    "        plt.title('Instruction Length Distribution')\n",
    "        plt.xlabel('Words')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.hist(stats['response_lengths'], bins=50, alpha=0.7)\n",
    "        plt.title('Response Length Distribution')\n",
    "        plt.xlabel('Words')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.boxplot([stats['instruction_lengths'], stats['response_lengths']], \n",
    "                   labels=['Instructions', 'Responses'])\n",
    "        plt.title('Length Comparison')\n",
    "        plt.ylabel('Words')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del stats['instruction_lengths']\n",
    "        del stats['response_lengths']\n",
    "        \n",
    "        return split_dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in dataset processing: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Dataset Structure:\n",
      "=========================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'context', 'response', 'category'],\n",
      "        num_rows: 15011\n",
      "    })\n",
      "})\n",
      "\n",
      "Dataset Statistics:\n",
      "==================\n",
      "Total examples: 15011\n",
      "Training examples: 13510\n",
      "Validation examples: 1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing examples: 100%|██████████| 13510/13510 [00:01<00:00, 10964.75it/s]\n",
      "/tmp/ipykernel_77078/4175369405.py:112: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([stats['instruction_lengths'], stats['response_lengths']],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average instruction length (words): 12.3\n",
      "Average response length (words): 60.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examining Random Examples:\n",
      "========================\n",
      "\n",
      "Example 1/3:\n",
      "========================================\n",
      "Category: general_qa\n",
      "\n",
      "Instruction:\n",
      "--------------------\n",
      "What is lucid dream and how can someone learn to do it?\n",
      "\n",
      "Response:\n",
      "--------------------\n",
      "Lucid dreaming is the ability to be consciously awake in your dreams. This allows you to make active decisions in your dreams and even shape the dream world. It is very useful for not just entertainment (e.g. who doesn't like to fly like a bird?) but also for self-help (e.g. practice facing your fears like public speaking) and discovery. To best way to get better at lucid dreaming is to keep a dream journal (writing down your dreams very morning). I also suggest you buy the original book \"Lucid Dreaming\" by Stephen LaBerge.\n",
      "\n",
      "Metrics:\n",
      "- Instruction length: 12 words\n",
      "- Response length: 93 words\n",
      "- Response/Instruction ratio: 7.75\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2/3:\n",
      "========================================\n",
      "Category: summarization\n",
      "\n",
      "Instruction:\n",
      "--------------------\n",
      "Provide me a 3 bulleted points summarizing the Los Angeles Lakers\n",
      "\n",
      "Response:\n",
      "--------------------\n",
      "The Los Angeles Lakers are one of the most storied franchises in the NBA (national basketball association) with a strong fan base following. Here are a few important points to know about the Lakers:\n",
      "- The lakers are a professional basketball team based in Los Angeles and play at the Crypto.com arena\n",
      "- the team has won 17 NBA championships, which is tied for most all time\n",
      "- the franchise initially started playing in Minnesota but relocated in 1960\n",
      "\n",
      "Metrics:\n",
      "- Instruction length: 11 words\n",
      "- Response length: 79 words\n",
      "- Response/Instruction ratio: 7.18\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3/3:\n",
      "========================================\n",
      "Category: summarization\n",
      "\n",
      "Instruction:\n",
      "--------------------\n",
      "Who are the almoravids ?\n",
      "\n",
      "Response:\n",
      "--------------------\n",
      "The Almoravid was imperial Berber Muslim Dynasty. They rulled in Morocco from 1050 until 1147 and Marrakech were their capital. They emerged from a coalition of the Lamtuna, Massufa and Gudala, nomad berbers l.\n",
      "\n",
      "Metrics:\n",
      "- Instruction length: 5 words\n",
      "- Response length: 34 words\n",
      "- Response/Instruction ratio: 6.80\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# First explore and split the dataset\n",
    "dataset = explore_dataset()\n",
    "\n",
    "# Then examine random examples\n",
    "examine_examples(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw format:\n",
      "===========\n",
      "### Human: What is machine learning?\n",
      "### Assistant: Machine learning is a branch of artificial intelligence...</s>\n",
      "\n",
      "Visible format:\n",
      "==============\n",
      "### Human: What is machine learning?\n",
      "### Assistant: Machine learning is a branch of artificial intelligence...<END>\n"
     ]
    }
   ],
   "source": [
    "def display_chat_format_example(instruction, response):\n",
    "    \"\"\"Show how the chat format looks.\"\"\"\n",
    "    formatted = f\"### Human: {instruction}\\n### Assistant: {response}</s>\"\n",
    "    print(\"Raw format:\")\n",
    "    print(\"===========\")\n",
    "    print(formatted)\n",
    "    print(\"\\nVisible format:\")\n",
    "    print(\"==============\")\n",
    "    print(formatted.replace(\"</s>\", \"<END>\"))  # Make the end token visible\n",
    "    return formatted\n",
    "\n",
    "# Example usage:\n",
    "example_instruction = \"What is machine learning?\"\n",
    "example_response = \"Machine learning is a branch of artificial intelligence...\"\n",
    "formatted_example = display_chat_format_example(example_instruction, example_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Information:\n",
      "=====================\n",
      "Vocabulary size: 128256\n",
      "Model max length: 131072\n",
      "BOS token: <|begin_of_text|> (id: 128000)\n",
      "EOS token: <|end_of_text|> (id: 128001)\n",
      "PAD token: <|end_of_text|> (id: 128001)\n",
      "\n",
      "Tokenization Example:\n",
      "===================\n",
      "Original text: Hello, how are you?\n",
      "Token IDs: [[128000, 9906, 11, 1268, 527, 499, 30]]\n",
      "Tokens: ['<|begin_of_text|>', 'Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']\n",
      "Decoded back: <|begin_of_text|>Hello, how are you?\n",
      "\n",
      "Attention Mask:\n",
      "[[1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "def explore_tokenizer(model_name=\"meta-llama/Llama-2-7b-chat\"):\n",
    "    \"\"\"Setup and explore the tokenizer behavior with proper configurations for LLaMA.\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # LLaMA specific configurations\n",
    "        # Setting pad_token to eos_token if not set\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "        print(\"Tokenizer Information:\")\n",
    "        print(\"=====================\")\n",
    "        print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "        print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "        print(f\"BOS token: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})\")\n",
    "        print(f\"EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\n",
    "        print(f\"PAD token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\n",
    "        \n",
    "        # Example tokenization with full details\n",
    "        example_text = \"Hello, how are you?\"\n",
    "        encoding = tokenizer(example_text, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        print(\"\\nTokenization Example:\")\n",
    "        print(\"===================\")\n",
    "        print(f\"Original text: {example_text}\")\n",
    "        print(f\"Token IDs: {encoding['input_ids'].tolist()}\")\n",
    "        print(f\"Tokens: {tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])}\")\n",
    "        print(f\"Decoded back: {tokenizer.decode(encoding['input_ids'][0])}\")\n",
    "        \n",
    "        # Show attention mask\n",
    "        print(\"\\nAttention Mask:\")\n",
    "        print(encoding['attention_mask'].tolist())\n",
    "        \n",
    "        return tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {str(e)}\")\n",
    "        print(\"Please ensure you have proper access to the model and try a different model if needed.\")\n",
    "        raise\n",
    "# Example usage:\n",
    "tokenizer = explore_tokenizer(\"meta-llama/Llama-3.2-1B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_and_tokenizer(model_name=MODEL_NAME):\n",
    "    \"\"\"Initialize model and tokenizer with proper memory management.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    try:\n",
    "        # Initialize tokenizer\n",
    "        tokenizer = explore_tokenizer(model_name)\n",
    "            \n",
    "        # Initialize model with memory optimizations\n",
    "        model_kwargs = {\n",
    "            \"torch_dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model_kwargs.update({\n",
    "                \"load_in_8bit\": True,\n",
    "                \"device_map\": \"auto\",\n",
    "            })\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "        \n",
    "        # Prepare model for training\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        \n",
    "        # Configure LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        # Get PEFT model\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(torch.bfloat16)\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing model and tokenizer: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Information:\n",
      "=====================\n",
      "Vocabulary size: 128256\n",
      "Model max length: 131072\n",
      "BOS token: <|begin_of_text|> (id: 128000)\n",
      "EOS token: <|end_of_text|> (id: 128001)\n",
      "PAD token: <|end_of_text|> (id: 128001)\n",
      "\n",
      "Tokenization Example:\n",
      "===================\n",
      "Original text: Hello, how are you?\n",
      "Token IDs: [[128000, 9906, 11, 1268, 527, 499, 30]]\n",
      "Tokens: ['<|begin_of_text|>', 'Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']\n",
      "Decoded back: <|begin_of_text|>Hello, how are you?\n",
      "\n",
      "Attention Mask:\n",
      "[[1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and tokenizer globally\n",
    "model, tokenizer = initialize_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Optimized dataset class for chat data.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Format chat text\n",
    "        chat_text = (\n",
    "            f\"### Human: {item['instruction']}\\n\"\n",
    "            f\"### Assistant: {item['response']}\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize with proper padding and truncation\n",
    "        encoded = self.tokenizer(\n",
    "            chat_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension and convert to proper dtypes\n",
    "        input_ids = encoded['input_ids'].squeeze(0).to(torch.long)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0).to(torch.long)\n",
    "        \n",
    "        # Create labels\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class CustomDataCollator:\n",
    "    def __call__(self, features):\n",
    "        batch = {\n",
    "            'input_ids': torch.stack([f['input_ids'].cpu() for f in features]).to(torch.long),\n",
    "            'attention_mask': torch.stack([f['attention_mask'].cpu() for f in features]).to(torch.long),\n",
    "            'labels': torch.stack([f['labels'].cpu() for f in features]).to(torch.long)\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing and exploring tokenizer...\n",
      "Tokenizer Information:\n",
      "=====================\n",
      "Vocabulary size: 128256\n",
      "Model max length: 131072\n",
      "BOS token: <|begin_of_text|> (id: 128000)\n",
      "EOS token: <|end_of_text|> (id: 128001)\n",
      "PAD token: <|end_of_text|> (id: 128001)\n",
      "\n",
      "Tokenization Example:\n",
      "===================\n",
      "Original text: Hello, how are you?\n",
      "Token IDs: [[128000, 9906, 11, 1268, 527, 499, 30]]\n",
      "Tokens: ['<|begin_of_text|>', 'Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']\n",
      "Decoded back: <|begin_of_text|>Hello, how are you?\n",
      "\n",
      "Attention Mask:\n",
      "[[1, 1, 1, 1, 1, 1, 1]]\n",
      "\n",
      "Exploring dataset processing...\n",
      "Dataset Processing Example:\n",
      "=========================\n",
      "\n",
      "Original text:\n",
      "Instruction: What is Python?\n",
      "Response: Python is a programming language.\n",
      "\n",
      "Tokenizer Configuration:\n",
      "Pad Token ID: 128001\n",
      "EOS Token ID: 128001\n",
      "BOS Token ID: 128000\n",
      "\n",
      "Processed data shapes:\n",
      "Input IDs shape: torch.Size([128])\n",
      "Attention mask shape: torch.Size([128])\n",
      "Labels shape: torch.Size([128])\n",
      "\n",
      "Input IDs:\n",
      "[128000, 14711, 11344, 25, 3639, 374, 13325, 5380, 14711, 22103, 25, 13325, 374, 264, 15840, 4221, 13, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001]\n",
      "\n",
      "Tokens:\n",
      "['<|begin_of_text|>', '###', ' Human', ':', ' What', ' is', ' Python', '?\\n', '###', ' Assistant', ':', ' Python', ' is', ' a', ' programming', ' language', '.']\n",
      "\n",
      "Attention Mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Labels:\n",
      "[128000, 14711, 11344, 25, 3639, 374, 13325, 5380, 14711, 22103, 25, 13325, 374, 264, 15840, 4221, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Decoded full text:\n",
      "<|begin_of_text|>### Human: What is Python?\n",
      "### Assistant: Python is a programming language.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Enhanced dataset exploration function\n",
    "def explore_dataset_processing(tokenizer):\n",
    "    \"\"\"Detailed exploration of how data is processed in the dataset.\"\"\"\n",
    "    example_data = [\n",
    "        {\n",
    "            'instruction': 'What is Python?',\n",
    "            'response': 'Python is a programming language.'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create dataset instance\n",
    "    dataset = ChatDataset(example_data, tokenizer, max_length=128)\n",
    "    \n",
    "    # Get first item\n",
    "    item = dataset[0]\n",
    "    \n",
    "    print(\"Dataset Processing Example:\")\n",
    "    print(\"=========================\")\n",
    "    \n",
    "    print(\"\\nOriginal text:\")\n",
    "    print(f\"Instruction: {example_data[0]['instruction']}\")\n",
    "    print(f\"Response: {example_data[0]['response']}\")\n",
    "    \n",
    "    print(\"\\nTokenizer Configuration:\")\n",
    "    print(f\"Pad Token ID: {tokenizer.pad_token_id}\")\n",
    "    print(f\"EOS Token ID: {tokenizer.eos_token_id}\")\n",
    "    print(f\"BOS Token ID: {tokenizer.bos_token_id if tokenizer.bos_token_id else 'None'}\")\n",
    "    \n",
    "    print(\"\\nProcessed data shapes:\")\n",
    "    print(f\"Input IDs shape: {item['input_ids'].shape}\")\n",
    "    print(f\"Attention mask shape: {item['attention_mask'].shape}\")\n",
    "    print(f\"Labels shape: {item['labels'].shape}\")\n",
    "    \n",
    "    print(\"\\nInput IDs:\")\n",
    "    print(item['input_ids'].tolist())\n",
    "    print(\"\\nTokens:\")\n",
    "    print([tokenizer.decode([tid]) for tid in item['input_ids'] if tid != tokenizer.pad_token_id])\n",
    "    \n",
    "    print(\"\\nAttention Mask:\")\n",
    "    print(item['attention_mask'].tolist())\n",
    "    \n",
    "    print(\"\\nLabels:\")\n",
    "    print(item['labels'].tolist())\n",
    "    \n",
    "    print(\"\\nDecoded full text:\")\n",
    "    decoded = tokenizer.decode(item['input_ids'])\n",
    "    print(decoded)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Usage example:\n",
    "print(\"Initializing and exploring tokenizer...\")\n",
    "tokenizer = explore_tokenizer(\"meta-llama/Llama-3.2-1B\")\n",
    "print(\"\\nExploring dataset processing...\")\n",
    "example_dataset = explore_dataset_processing(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_args(output_dir=\"./results\", num_epochs=3):\n",
    "    \"\"\"Setup training arguments with stabilized training parameters.\"\"\"\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        # Reduce batch size and increase gradient accumulation\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=32,  # Increased from 16\n",
    "        \n",
    "        # Better learning rate schedule\n",
    "        learning_rate=1e-5,  # Reduced from 2e-5\n",
    "        warmup_ratio=0.1,    # Increased from 0.03\n",
    "        lr_scheduler_type=\"cosine\",  # Added cosine schedule\n",
    "        \n",
    "        # Gradient clipping and weight decay\n",
    "        max_grad_norm=0.3,   # Reduced from 0.5\n",
    "        weight_decay=0.05,   # Added weight decay\n",
    "        \n",
    "        # Evaluation strategy\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,       # More frequent evaluation\n",
    "        logging_steps=10,\n",
    "        save_steps=50,\n",
    "        save_total_limit=2,\n",
    "        \n",
    "        # Precision settings\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        bf16_full_eval=True,\n",
    "        \n",
    "        # Other settings\n",
    "        gradient_checkpointing=True,  # Enable to save memory\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        optim=\"adamw_8bit\",\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=0,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, dataset, training_args=None):\n",
    "    \"\"\"Main training function with memory management.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    try:\n",
    "        # Create datasets\n",
    "        train_dataset = ChatDataset(dataset['train'], tokenizer)\n",
    "        val_dataset = ChatDataset(dataset['validation'], tokenizer)\n",
    "        \n",
    "        print(f\"\\nDataset sizes:\")\n",
    "        print(f\"Train: {len(train_dataset)}\")\n",
    "        print(f\"Validation: {len(val_dataset)}\")\n",
    "        \n",
    "        # Use default training args if none provided\n",
    "        if training_args is None:\n",
    "            training_args = setup_training_args()\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=CustomDataCollator(),\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        print(\"\\nStarting training...\")\n",
    "        print(\"Model device:\", next(model.parameters()).device)\n",
    "        print(\"Model dtype:\", next(model.parameters()).dtype)\n",
    "        print(f\"Starting GPU Memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "        \n",
    "        result = trainer.train()\n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        \n",
    "        return trainer, result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining error: {str(e)}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory at error: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, tokenizer, prompts, max_new_tokens=100):\n",
    "    \"\"\"Test model with memory management.\"\"\"\n",
    "    try:\n",
    "        model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        results = []\n",
    "        with torch.no_grad():\n",
    "            for prompt in prompts:\n",
    "                formatted_prompt = f\"### Human: {prompt}\\n### Assistant:\"\n",
    "                inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=0.7,\n",
    "                    num_return_sequences=1,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                results.append((prompt, response))\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    finally:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/home/uumami/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('What is deep learning?',\n",
       "  '### Human: What is deep learning?\\n### Assistant: What is deep learning?\\n### Deep learning is the latest technology for image recognition, speech recognition, and natural language processing.\\n\\n### Deep learning is a machine learning technique that involves a network of neurons that is trained on a dataset. These neurons are connected to each other in a specific way, and they are able to learn and generalize from the data they are given. Deep learning is a type of artificial intelligence that uses a large number of interconnected nodes to process data. It is used in a wide variety of')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "test_prompts = [\n",
    "    \"What is deep learning?\",\n",
    "    #\"Explain how neural networks work.\",\n",
    "    #\"What is the difference between supervised and unsupervised learning?\"\n",
    "]\n",
    "\n",
    "test_model(model, tokenizer, test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_name=\"databricks/databricks-dolly-15k\", validation_split=0.1):\n",
    "    \"\"\"Load and prepare the dataset.\"\"\"\n",
    "    try:\n",
    "        # Load dataset\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        print(\"\\nLoaded dataset structure:\")\n",
    "        print(dataset)\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        total_size = len(dataset['train'])\n",
    "        val_size = int(total_size * validation_split)\n",
    "        train_size = total_size - val_size\n",
    "        \n",
    "        # Split dataset\n",
    "        splits = dataset['train'].train_test_split(\n",
    "            test_size=validation_split, \n",
    "            shuffle=True, \n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Create proper format\n",
    "        prepared_dataset = {\n",
    "            'train': splits['train'],\n",
    "            'validation': splits['test']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nDataset splits:\")\n",
    "        print(f\"Training samples: {len(prepared_dataset['train'])}\")\n",
    "        print(f\"Validation samples: {len(prepared_dataset['validation'])}\")\n",
    "        \n",
    "        return prepared_dataset\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing dataset: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_pipeline(validation_split=0.1, num_epochs=3):\n",
    "    \"\"\"Complete training pipeline using all modular components.\"\"\"\n",
    "    try:\n",
    "        # Initialize model and tokenizer\n",
    "        model, tokenizer = initialize_model_and_tokenizer()\n",
    "        \n",
    "        # Prepare dataset\n",
    "        print(\"\\nPreparing dataset...\")\n",
    "        dataset = prepare_dataset(validation_split=validation_split)\n",
    "        \n",
    "        # Setup training arguments\n",
    "        training_args = setup_training_args(\n",
    "            output_dir=\"./results\",\n",
    "            num_epochs=num_epochs\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        trainer, results = train_model(model, tokenizer, dataset, training_args)\n",
    "        \n",
    "        # Save the model\n",
    "        trainer.save_model(\"./final_model\")\n",
    "        \n",
    "        # Test the model\n",
    "        test_prompts = [\n",
    "            \"What is machine learning?\",\n",
    "            \"Explain how neural networks work.\"\n",
    "        ]\n",
    "        test_results = test_model(model, tokenizer, test_prompts)\n",
    "        \n",
    "        # Print test results\n",
    "        print(\"\\nTest Results:\")\n",
    "        for prompt, response in test_results:\n",
    "            print(f\"\\nPrompt: {prompt}\")\n",
    "            print(f\"Response: {response}\")\n",
    "        \n",
    "        return trainer, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nPipeline error: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline\n",
    "trainer, results = run_training_pipeline(\n",
    "    validation_split=0.1,\n",
    "    num_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Memory: 1456.43 MB\n",
      "\n",
      "Preparing dataset...\n",
      "\n",
      "Loaded dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'context', 'response', 'category'],\n",
      "        num_rows: 15011\n",
      "    })\n",
      "})\n",
      "\n",
      "Dataset splits:\n",
      "Training samples: 13509\n",
      "Validation samples: 1502\n",
      "\n",
      "Dataset sizes:\n",
      "Train: 13509\n",
      "Validation: 1502\n",
      "\n",
      "Starting training...\n",
      "Model device: cuda:0\n",
      "Model dtype: torch.bfloat16\n",
      "Starting GPU Memory: 1453.12 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uumami/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891de82634a142a7ba91536235658da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uumami/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/uumami/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/uumami/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4139, 'grad_norm': 1.546875, 'learning_rate': 1.9999012930609023e-05, 'epoch': 0.0}\n",
      "{'loss': 3.0806, 'grad_norm': 1.1953125, 'learning_rate': 1.9998025861218044e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4824, 'grad_norm': 2.78125, 'learning_rate': 1.999703879182707e-05, 'epoch': 0.0}\n",
      "{'loss': 2.3351, 'grad_norm': 1.2109375, 'learning_rate': 1.999605172243609e-05, 'epoch': 0.0}\n",
      "{'loss': 4.2956, 'grad_norm': 2.90625, 'learning_rate': 1.9995064653045112e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea44920c180942528614dd50963bf934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory cleaned up\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel dtype:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting GPU Memory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 6. Save the final model\u001b[39;00m\n\u001b[1;32m     44\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./final_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2366\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2366\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2814\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2812\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2814\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2771\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2771\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2772\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2774\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3676\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3673\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3675\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3676\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3679\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3680\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3684\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3686\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3867\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3864\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3866\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3867\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3868\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3869\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:4085\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   4084\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4085\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4086\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   4088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3373\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3372\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3373\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3374\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3375\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    989\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    990\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         position_embeddings,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:750\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    753\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:309\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    307\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:1009\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 1009\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights:\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCxB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;66;03m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m         \u001b[38;5;66;03m# we no longer need the row-major weight\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:556\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    555\u001b[0m     state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    320\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 321\u001b[0m CA, CAt, SCA, SCAt, coo_tensorA \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m coo_tensorA \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mhas_fp16_weights:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/functional.py:2584\u001b[0m, in \u001b[0;36mdouble_quant\u001b[0;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001b[0m\n\u001b[1;32m   2582\u001b[0m is_on_gpu([A, col_stats, row_stats, out_col, out_row])\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     nnz \u001b[38;5;241m=\u001b[39m \u001b[43mnnz_row_ptr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nnz \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2586\u001b[0m         coo_tensor \u001b[38;5;241m=\u001b[39m coo_zeros(A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], nnz_row_ptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(), device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training execution\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "try:\n",
    "    # 1. Load and prepare dataset\n",
    "    print(\"\\nPreparing dataset...\")\n",
    "    full_dataset = prepare_dataset(validation_split=0.1)\n",
    "    \n",
    "    # 2. Create datasets using our ChatDataset class\n",
    "    train_dataset = ChatDataset(full_dataset['train'], tokenizer)\n",
    "    val_dataset = ChatDataset(full_dataset['validation'], tokenizer)\n",
    "    \n",
    "    print(f\"\\nDataset sizes:\")\n",
    "    print(f\"Train: {len(train_dataset)}\")\n",
    "    print(f\"Validation: {len(val_dataset)}\")\n",
    "    \n",
    "    # 3. Setup training arguments using our function\n",
    "    training_args = setup_training_args(\n",
    "        output_dir=\"./results\",\n",
    "        num_epochs=3  # Adjust as needed\n",
    "    )\n",
    "    \n",
    "    # 4. Initialize trainer with our custom data collator\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=CustomDataCollator(),\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # 5. Start training\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"Model device:\", next(model.parameters()).device)\n",
    "    print(\"Model dtype:\", next(model.parameters()).dtype)\n",
    "    print(f\"Starting GPU Memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    result = trainer.train()\n",
    "    \n",
    "    # 6. Save the final model\n",
    "    trainer.save_model(\"./final_model\")\n",
    "    print(\"\\nTraining completed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nDetailed error information:\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory at error: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"\\nMemory cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
